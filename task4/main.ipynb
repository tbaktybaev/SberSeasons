{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfX605Is7aMg"
      },
      "source": [
        "В колл-центре СЧАСТЬЕ работают операторы, отвечающие на звонки.\n",
        "К/ц СЧАСТЬЕ передает нам информацию о звонках в следующем виде.\n",
        " \n",
        "В таблицу CALLS к/ц записывает информацию о звонках. Колонки таблицы:\n",
        "1. date - дата звонка, строка в формате \"YYYY-MM-DD\"\n",
        "2. call_id - идентификатор звонка, строка\n",
        "3. audio_file - имя файла с записью звонка, строка\n",
        "4. oper_id - идентификатор оператора, ответившего на звонок, строка\n",
        " \n",
        "При этом данные о звонках могут поступать не сразу, а с задержкой в несколько дней,\n",
        "поэтому ведется таблица DATES, в которую к/ц записывает даты, за которые пришли новые данные о звонках.\n",
        "В таблице DATES  единственная колонка:\n",
        "1. date - дата, за которую пришла информация, строка в том же формате \"YYYY-MM-DD\"\n",
        " \n",
        "Требуется написать код с использованием spark DataFrame API (этот код будут запускать ежедневно),\n",
        "который дописывает данные в две таблицы:\n",
        "\n",
        "1. STAT_TOTAL (колонки 1. date, 2. count, 3. stat_date_time)\n",
        "- количество записей в таблице CALLS за даты из таблицы DATES\n",
        "- Примечание. stat_date_time - время запуска расчетов, т.е. текущее дата-время\n",
        "- Примечание. Если за некую дату нет звонков, то НУЖНО создать запись со значением 0 в колонке count\n",
        "\n",
        "2. STAT_OPER (колонки 1. date, 2. oper_id, 3. count, 4. stat_date_time)\n",
        "- количество записей в таблице CALLS за даты из таблицы DATES для каждого оператора\n",
        "- Примечание. Если за некую дату у некого оператора нет звонков, то НЕ НУЖНО создавать запись со значением 0 в колонке count\n",
        "\n",
        "После добавления данных в описанные выше две таблицы, требуется очистить таблицу DATES\n",
        "```\n",
        "pip install pyspark\n",
        "import pyspark\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import SparkSession\n",
        "from typing import List\n",
        "from pandas import DataFrame\n",
        "spark = SparkSession.builder.config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()\n",
        "dates = spark.createDataFrame([\n",
        "    Row(date='2022-01-20'),\n",
        "    Row(date='2022-01-18'),\n",
        "    Row(date='2022-01-16'),\n",
        "])\n",
        "calls = spark.createDataFrame([\n",
        "    Row(date='2022-01-20',  call_id='ERCERCER', audio_file='rec_12536123.wav',  oper_id=134),\n",
        "    Row(date='2022-01-20',  call_id='34FEWEC3', audio_file='rec_89372934.wav',  oper_id=134),\n",
        "    Row(date='2022-01-20',  call_id='ERC34F3E', audio_file='rec_56756775.wav',  oper_id=134),\n",
        "    Row(date='2022-01-20',  call_id='ERCJNER8', audio_file='rec_32454565.wav',  oper_id=128),\n",
        "    Row(date='2022-01-20',  call_id='ERLHCRE8', audio_file='rec_34545567.wav',  oper_id=125),\n",
        "    Row(date='2022-01-20',  call_id='LKECRE9C', audio_file='rec_23434564.wav',  oper_id=125),\n",
        "    Row(date='2022-01-19',  call_id='LJC8ER24', audio_file='rec_65778978.wav',  oper_id=127),\n",
        "    Row(date='2022-01-19',  call_id='KJNDFC94', audio_file='rec_34545766.wav',  oper_id=128),\n",
        "    Row(date='2022-01-19',  call_id='KJDC9833', audio_file='rec_34545656.wav',  oper_id=125),\n",
        "    Row(date='2022-01-19',  call_id='JHB38743', audio_file='rec_23434545.wav',  oper_id=125),\n",
        "    Row(date='2022-01-19',  call_id='U7JH76H5', audio_file='rec_56767876.wav',  oper_id=127),\n",
        "    Row(date='2022-01-18',  call_id='34F345F4', audio_file='rec_56567678.wav',  oper_id=134),\n",
        "    Row(date='2022-01-18',  call_id='WED34F45', audio_file='rec_34534534.wav',  oper_id=134),\n",
        "    Row(date='2022-01-18',  call_id='W3D34F56', audio_file='rec_56756767.wav',  oper_id=134),\n",
        "    Row(date='2022-01-18',  call_id='WF435F55', audio_file='rec_23434534.wav',  oper_id=116),\n",
        "    Row(date='2022-01-17',  call_id='NKDBUS83', audio_file='rec_13434876.wav',  oper_id=134),\n",
        "    Row(date='2022-01-17',  call_id='NBE83642', audio_file='rec_13434468.wav',  oper_id=116),\n",
        "    Row(date='2022-01-17',  call_id='NVID49DF', audio_file='rec_13434111.wav',  oper_id=134),\n",
        "])\n",
        "calls.show()\n",
        "dates.show()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLqEHTZV1y6I",
        "outputId": "678c2e26-8ced-42d6-87b6-b4036ee4ab1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824028 sha256=d977166296c8cf35ad60885a09cc015be6a92c084b212072f45a8cb8b62ef6aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuqZb9Vd1ve5",
        "outputId": "e9cc61a8-4c02-4920-be81-b44fd5e39fc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------+----------------+-------+\n",
            "|      date| call_id|      audio_file|oper_id|\n",
            "+----------+--------+----------------+-------+\n",
            "|2022-01-20|ERCERCER|rec_12536123.wav|    134|\n",
            "|2022-01-20|34FEWEC3|rec_89372934.wav|    134|\n",
            "|2022-01-20|ERC34F3E|rec_56756775.wav|    134|\n",
            "|2022-01-20|ERCJNER8|rec_32454565.wav|    128|\n",
            "|2022-01-20|ERLHCRE8|rec_34545567.wav|    125|\n",
            "|2022-01-20|LKECRE9C|rec_23434564.wav|    125|\n",
            "|2022-01-19|LJC8ER24|rec_65778978.wav|    127|\n",
            "|2022-01-19|KJNDFC94|rec_34545766.wav|    128|\n",
            "|2022-01-19|KJDC9833|rec_34545656.wav|    125|\n",
            "|2022-01-19|JHB38743|rec_23434545.wav|    125|\n",
            "|2022-01-19|U7JH76H5|rec_56767876.wav|    127|\n",
            "|2022-01-18|34F345F4|rec_56567678.wav|    134|\n",
            "|2022-01-18|WED34F45|rec_34534534.wav|    134|\n",
            "|2022-01-18|W3D34F56|rec_56756767.wav|    134|\n",
            "|2022-01-18|WF435F55|rec_23434534.wav|    116|\n",
            "|2022-01-17|NKDBUS83|rec_13434876.wav|    134|\n",
            "|2022-01-17|NBE83642|rec_13434468.wav|    116|\n",
            "|2022-01-17|NVID49DF|rec_13434111.wav|    134|\n",
            "+----------+--------+----------------+-------+\n",
            "\n",
            "+----------+\n",
            "|      date|\n",
            "+----------+\n",
            "|2022-01-20|\n",
            "|2022-01-18|\n",
            "|2022-01-16|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import SparkSession\n",
        "from typing import List\n",
        "from pandas import DataFrame\n",
        "\n",
        "spark = SparkSession.builder.config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()\n",
        "dates = spark.createDataFrame([\n",
        "    Row(date='2022-01-20'),\n",
        "    Row(date='2022-01-18'),\n",
        "    Row(date='2022-01-16'),\n",
        "])\n",
        "calls = spark.createDataFrame([\n",
        "    Row(date='2022-01-20',  call_id='ERCERCER', audio_file='rec_12536123.wav',  oper_id=134),\n",
        "    Row(date='2022-01-20',  call_id='34FEWEC3', audio_file='rec_89372934.wav',  oper_id=134),\n",
        "    Row(date='2022-01-20',  call_id='ERC34F3E', audio_file='rec_56756775.wav',  oper_id=134),\n",
        "    Row(date='2022-01-20',  call_id='ERCJNER8', audio_file='rec_32454565.wav',  oper_id=128),\n",
        "    Row(date='2022-01-20',  call_id='ERLHCRE8', audio_file='rec_34545567.wav',  oper_id=125),\n",
        "    Row(date='2022-01-20',  call_id='LKECRE9C', audio_file='rec_23434564.wav',  oper_id=125),\n",
        "    Row(date='2022-01-19',  call_id='LJC8ER24', audio_file='rec_65778978.wav',  oper_id=127),\n",
        "    Row(date='2022-01-19',  call_id='KJNDFC94', audio_file='rec_34545766.wav',  oper_id=128),\n",
        "    Row(date='2022-01-19',  call_id='KJDC9833', audio_file='rec_34545656.wav',  oper_id=125),\n",
        "    Row(date='2022-01-19',  call_id='JHB38743', audio_file='rec_23434545.wav',  oper_id=125),\n",
        "    Row(date='2022-01-19',  call_id='U7JH76H5', audio_file='rec_56767876.wav',  oper_id=127),\n",
        "    Row(date='2022-01-18',  call_id='34F345F4', audio_file='rec_56567678.wav',  oper_id=134),\n",
        "    Row(date='2022-01-18',  call_id='WED34F45', audio_file='rec_34534534.wav',  oper_id=134),\n",
        "    Row(date='2022-01-18',  call_id='W3D34F56', audio_file='rec_56756767.wav',  oper_id=134),\n",
        "    Row(date='2022-01-18',  call_id='WF435F55', audio_file='rec_23434534.wav',  oper_id=116),\n",
        "    Row(date='2022-01-17',  call_id='NKDBUS83', audio_file='rec_13434876.wav',  oper_id=134),\n",
        "    Row(date='2022-01-17',  call_id='NBE83642', audio_file='rec_13434468.wav',  oper_id=116),\n",
        "    Row(date='2022-01-17',  call_id='NVID49DF', audio_file='rec_13434111.wav',  oper_id=134),\n",
        "])\n",
        "\n",
        "calls.show()\n",
        "dates.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHwp5F3o6c6W",
        "outputId": "c9a0123f-2cfe-4547-9705-6ca71f3e52e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----+--------------------+\n",
            "|      date|count|      stat_date_time|\n",
            "+----------+-----+--------------------+\n",
            "|2022-01-20|    6|2023-04-12 04:42:...|\n",
            "|2022-01-18|    4|2023-04-12 04:42:...|\n",
            "|2022-01-16|    0|2023-04-12 04:42:...|\n",
            "+----------+-----+--------------------+\n",
            "\n",
            "+----------+-------+-----+--------------------+\n",
            "|      date|oper_id|count|      stat_date_time|\n",
            "+----------+-------+-----+--------------------+\n",
            "|2022-01-20|    134|    3|2023-04-12 04:42:...|\n",
            "|2022-01-20|    128|    1|2023-04-12 04:42:...|\n",
            "|2022-01-20|    125|    2|2023-04-12 04:42:...|\n",
            "|2022-01-18|    134|    3|2023-04-12 04:42:...|\n",
            "|2022-01-18|    116|    1|2023-04-12 04:42:...|\n",
            "+----------+-------+-----+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# создание таблицы \n",
        "schema = StructType([\n",
        "    StructField(\"date\", StringType(), True),\n",
        "    StructField(\"count\", IntegerType(), True),\n",
        "    StructField(\"stat_date_time\", TimestampType(), True)\n",
        "])\n",
        "stat_total = spark.createDataFrame([], schema)\n",
        "\n",
        "# создание таблицы \n",
        "schema = StructType([\n",
        "    StructField(\"date\", StringType(), True),\n",
        "    StructField(\"oper_id\", StringType(), True),\n",
        "    StructField(\"count\", IntegerType(), True),\n",
        "    StructField(\"stat_date_time\", TimestampType(), True)\n",
        "])\n",
        "stat_oper = spark.createDataFrame([], schema)\n",
        "# подсчет количества записей из данных таблиц\n",
        "calls = calls.withColumn(\"date\", F.to_date(\"date\"))\n",
        "dates = dates.withColumn(\"date\", F.to_date(\"date\"))\n",
        "\n",
        "# получение уникальных значений из таблиц\n",
        "distinct_calls_dates = calls.select(\"date\").distinct()\n",
        "distinct_dates = dates.select(\"date\").distinct()\n",
        "\n",
        "count_by_date = calls.join(dates, \"date\") \\\n",
        "    .groupBy(\"date\") \\\n",
        "    .agg(F.count(\"*\").alias(\"count\")) \\\n",
        "    .withColumn(\"stat_date_time\", F.current_timestamp())\n",
        "\n",
        "# получение даты, которые есть в таблице dates, но которых нет в таблице calls\n",
        "missing_dates = distinct_dates.subtract(distinct_calls_dates)\n",
        "\n",
        "# создание записей со значением 0 в колонке count для отсутствующих дат\n",
        "missing_count_by_date = missing_dates \\\n",
        "    .withColumn(\"count\", F.lit(0)) \\\n",
        "    .withColumn(\"stat_date_time\", F.current_timestamp())\n",
        "    \n",
        "# добавление их в таблицу stat_total\n",
        "stat_total = count_by_date.union(missing_count_by_date)\n",
        "\n",
        "# подсчет количества записей в таблице calls за даты из таблицы DATES для каждого оператора\n",
        "count_by_oper_date = calls.join(dates, \"date\") \\\n",
        "    .groupBy(\"date\", \"oper_id\") \\\n",
        "    .agg(F.count(\"*\").alias(\"count\")) \\\n",
        "    .withColumn(\"stat_date_time\", F.current_timestamp())\n",
        "\n",
        "# добавление данных в таблицу stat_oper \n",
        "stat_oper = stat_oper.union(count_by_oper_date)\n",
        "stat_total.show()\n",
        "stat_oper.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odlP_6cv-dCH",
        "outputId": "c627cbff-b719-4fc1-8de2-d87dc7eb6ffe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+\n",
            "|date|\n",
            "+----+\n",
            "+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# очистка таблицы dates \n",
        "dates = spark.createDataFrame([], dates.schema)\n",
        "dates.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
